
\lhead[\chaptername~\thechapter]{\rightmark}

\rhead[\leftmark]{}

\lfoot[\thepage]{}

\cfoot{}

\rfoot[]{\thepage}

\chapter{Methods}

This chapter first starts by providing a survey of existing methods
that address the problem of detecting changes in a system. Later on,
general information about Markov chains, the simple Markov switching
model feature, and model specification namely Markov switching autoregressive
model are discussed. Thereafter, three sections are devoted to methods
for estimating the values of parameters, predicting a state for a
new observation, and selecting a suitable model for the datasets.
Another change-point method in a non-parametric approach is described.
Finally, the simulation technique is explained.

\section{Survey of existing methods}

\emph{Change point detection, Anomaly detection, Intrusion detection,
}or \emph{Outlier detection} are terms that are closely related to
one another. The main idea of these terms is to identify and discover
events that are abnormal from the usual behavior. There are several
methods to address these types of problem. A survey of existing methods
has been done in the thesis, and some methods are presented in this
section.

\citet{valdes2000adaptive} employed a Bayesian inference technique,
specifically a naive Bayesian network, to create an intrusion detection
system on traffic bursts. Even though the Bayesian network is effective
in detecting anomalies in some applications, there are some limitations
that should be considered when using this method. As accuracy of a
detection system depends heavily on certain assumptions, the system
will have low accuracy if an inaccurate model is implemented \citep{patcha2007overview}.

Support vector machine (SVM) introduced in \citet{cortes1995support}
is a supervised learning algorithm to deal with a classification analysis
problem by using the idea of separating hyperplanes. The main reason
that SVM is used in anomaly detection is because of its speed and
scalability \citep{sung2003identifying}. Although this method is
effective in identifying new kinds of anomalies, the method often
has a higher rate of false alarms due to the fact that the SVM method
ignores the relationships and dependencies between the features \citep{sarasamma2005hierarchical}. 

Self-organizing maps (SOM) developed by \citet{kohonen1982self} is
a well-known unsupervised neural network approach for cluster analysis.
SOM is efficient in handling large and high dimensional datasets.
\citet{nousiainen2009anomaly} used SOM for an anomaly detection in
a server log data. The study presented an ability of the SOM method
in detecting anomalies in the data, and also compared the results
from the SOM method with a threshold based system. A disadvantage
of the SOM is that initial weight vector affects a performance of
the SOM, which leads to an unstable clustering result. Besides, if
the anomalies in the data tend to form clusters, this method will
not be able to detect these anomalies \citep{chandola2009anomaly}. 

Based on previous works, the Hidden Markov model or the Markov switching
model has also been used in identifying changes and anomalies. One
drawback from the method based on the Markov chain is that the method
has a high computational cost, which is not scalable for an online
change application \citep{patcha2007overview}. Apart from changes
that can be detected in the data, some knowledge about the unobservable
condition of the system can also be obtained. This additional information
makes the method more appealing than the other methods. Therefore,
the Markov switching model is implemented in this thesis framework. 

\section{Technical aspects}

The thesis work has been carried out using the \emph{R} programming
language \citep{rprogram} for the purpose of data cleaning, preprocessing,
and analysis. The Markov switching model was performed using the \emph{MSwM}
package. Various extensions and modifications were further implemented
in the package e.g., handling predictor categorical variables, the
state prediction function, and plots for visualizing the results (More
details can be found in \ref{sec:MSwM-Package}). For the E-divisive
method, the \emph{ecp} package was used. 

\section{Markov chains\label{sec:Markov-chains}}

A Markov chain is a random process which has a property that is given
the current value, the future is independent of the past. A random
process $X$ contains random variables $X_{t}:\:t\in T$ indexed by
a set $T$. When $T=\{0,1,2,...\}$ the process is called a discrete-time
process, and when $T=[0,\infty)$ it is called a continuous-time process.
Let $X_{t}$ be a sequence of values from a state space $S$. The
process begins from one of these states and moves to another state.
The move between states is called a step. The process of Markov chains
is described here.
\begin{defn}
\citep[p.214]{grimmett2001probability} If a process $X$ satisfies
the Markov property, the process $X$ is a first order Markov chain

\[
P(X_{t}=s|X_{0}=x_{0},X_{1}=x_{1},...,X_{t-1}=x_{t-1})=P(X_{t}=s|X_{t-1}=x_{t-1})
\]

where $t\ge1$ and $s,x_{0},...,x_{t-1}\in S$
\end{defn}
If $X_{t}=i$ then the chain is in state $i$ or the chain is in the
$i$th state at the $t$th step. 

There are transitions between states which describe the distribution
of the next state given the current state. The evolution of changing
from $X_{t}=i$ to $X_{t}=j$ is defined as the transition probability
$P(X_{t}=j|X_{t-1}=i)$. For Markov chains, it is frequently assumed
that these probabilities depend only on $i$ and $j$ and do not depend
on $t$.
\begin{defn}
\citep[p.214]{grimmett2001probability} A Markov chain is \emph{time-homogeneous}
if

\[
P(X_{t+1}=j|X_{t}=i)=P(X_{1}=j|X_{0}=i)
\]

for all $t,i,j$. The probability of the transition is independent
of $t$. A \emph{transition matrix} $\mathbf{P}=(p_{ij})$ is a matrix
of transition probabilities 

\[
p_{ij}=P(X_{t}=j|X_{t-1}=i)\qquad\mathrm{for}\,\mathrm{all\,}t,i,j
\]
\end{defn}
\begin{thm*}
\citep[p.215]{grimmett2001probability} The transition matrix $\mathbf{P}$
is a matrix that
\begin{itemize}
\item Each of the entries is a non-negative real number or $p_{ij}\ge0$
for all $i,j$
\item The sum of each row equal to one or $\sum_{j}p_{ij}=1$ for all $i$
\end{itemize}
\end{thm*}

\begin{defn}
\label{def:-The-vector}\citep[p.227]{grimmett2001probability} The
vector $\pi$ is called a \emph{stationary distribution} if $\pi$
has entries $(\pi_{j}:\:j\in S)$ that satisfies
\begin{itemize}
\item $\pi_{j}\geq0$ for all $j$, and $\sum_{j}\pi_{j}=1$
\item $\pi=\pi\mathbf{P}$, which is $\pi_{j}=\sum_{i}\pi_{i}p_{ij}$ for
all $j$
\end{itemize}
\end{defn}

\begin{defn}
\citep[p.220]{grimmett2001probability} A state $i$ is called persistent
(or recurrent) if

\[
P(X_{t}=i\mathrm{\:for\,}\mathrm{some\,\mathrm{t}}\geq1|X_{0}=i)=1
\]
\end{defn}

Let $f_{ij}(t)=P(X_{1}\neq j,X_{2}\neq j,...,X_{t}=j|X_{0}=i)$ be
the probability of visiting state $j$ first by starting from $i$,
takes place at $t$th step.

\begin{defn}
\citep[p.222]{grimmett2001probability} The mean recurrence time of
a persistent state $i$ is defined as

\[
\mu_{i}=E(T_{i}|X_{0}=i)=\sum_{n}n\cdot f_{ii}(n)
\]

A persistent state $i$ is non-null (or positive recurrent) if $\mu_{i}$
is finite. Otherwise, the persistent state $i$ is null. 
\end{defn}

\begin{defn}
\citep[p.222]{grimmett2001probability} The period $d(i)$ of a state
$i$ is defined as 

\[
d(i)=gcd\{n:\:p_{ii}(n)>0\}
\]

where $gcd$ is the greatest common divisor. If $d(i)=1$, then the
state is said to be aperiodic. Otherwise, the state is said to be
periodic. 
\end{defn}

\begin{defn}
\citep[p.222]{grimmett2001probability} A state is called ergodic
if it is non-null persistent and aperiodic.
\end{defn}

\begin{defn}
A chain is called irreducible if it is possible to go from every state
to every other states.
\end{defn}

\begin{thm*}
If all states in an irreducible Markov chain are ergodic, the chain
is said to be ergodic.
\end{thm*}

\begin{thm*}
\citep{manning2008introduction} If there is an aperiodic finite state,
an irreducible Markov chain is the same thing as an ergodic Markov
chain. 
\end{thm*}

\section{Markov switching model}

A Markov switching model is a switching model where the shifting back
and forth between the states or regimes is controlled by a latent
Markov chain. The model structure consists of two stochastic processes
embedded in two levels of hierarchy. One process is an underlying
stochastic process that is not normally observable, but possible to
be observed through another stochastic process which generates the
sequence of observation \citep{rabiner1986introduction}. The transition
time between two states is random. In addition, the state are assumed
to follow the Markov property that the future state depends only on
the current state. 

The Markov switching model is able to model more complex stochastic
processes and describe changes in the dynamic behavior. A general
structure of the model can be drawn graphically as shown in \ref{msm},
where $S_{t}$ and $y_{t}$ denote the state sequence and observation
sequence in the Markov process, respectively. The arrows from one
state to another state in the diagram implies a conditional dependency. 

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.6]{picture/msm1}
\par\end{centering}
\caption{Model structure}
\label{msm}
\end{figure}

The process is given by \citep{hamilton1989new}

\begin{equation}
y_{t}=X_{t}\beta_{S_{t}}+\varepsilon_{t}\label{eq:general_mswm}
\end{equation}
where, 
\begin{labeling}{00.00.0000}
\item [{$y_{t}$}] is an observed value of the time series at time $t$
\item [{$X_{t}$}] is a design matrix, also known as model matrix, containing
values of predictor variables of the time series at time $t$
\item [{$\beta_{S_{t}}$}] are a column vector of coefficients in state
$S_{t}$, where $S_{t}\in\{1,...,k\}$ 
\item [{$\varepsilon_{t}$}] follows a normal distribution with zero mean
and variance given by $\sigma_{S_{t}}^{2}$ 
\end{labeling}
Equation \ref{eq:general_mswm} is the simplest form for the switching
model. To aid understanding, the baseline model is assumed to have
only two states $(k=2)$ in this discussion. $S_{t}$ is a random
variable which is assumed that the value $S_{t}=1$ for $t=1,2,...,t_{0}$
and $S_{t}=2$ for $t=t_{0}+1,t_{0}+2,...,T$ where $t_{0}$ is a
known change point. 

The transition matrix $\mathbf{P}$ is an $2\mathrm{x}2$ matrix where
row $j$ column $i$ element is the transition probability $p_{ij}$.
A diagram showing a state-transition is shown in \ref{transition}.
Note that these probabilities are independent of $t$.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.6]{picture/transition}
\par\end{centering}
\caption{State-transition diagram}
\label{transition}
\end{figure}

Since the whole process $S_{t}$ is unobserved, the initial state
where $t=0$ also needs to be specified. The probability which describes
the starting distribution over states is denoted by

\[
\pi_{i}=P(S_{0}=i)
\]

There are several options for computing the probability of the initial
state. One procedure is to commonly set $P(S_{0}=i)=0.5$. Alternatively,
by presuming an ergodic Markov chain \citep{hamilton2005regime},
a stationary distribution is 

\[
\pi_{i}=P(S_{0}=i)=\frac{1-p_{jj}}{2-p_{ii}-p_{jj}}
\]

which is simply from solving the system of equations $\pi=\pi\mathbf{P}$. 
\begin{proof}
Let $\pi=(\pi_{1},\pi_{2})'$ and $\mathbf{P}=\left[\begin{array}{cc}
p_{ii} & 1-p_{ii}\\
1-p_{jj} & p_{jj}
\end{array}\right]$

Definition\ref{def:-The-vector} of a stationary distribution, 
\begin{eqnarray}
\pi & = & \pi\mathbf{P}\label{eq:1}
\end{eqnarray}
and
\begin{eqnarray}
\pi_{1}+\pi_{2} & = & 1\label{eq:2}
\end{eqnarray}
From \ref{eq:1},
\begin{eqnarray*}
\pi_{1} & = & \pi_{1}p_{ii}+\pi_{2}(1-p_{jj})\\
\pi_{2} & = & \pi_{1}(1-p_{ii})+\pi_{2}p_{jj}
\end{eqnarray*}
Therefore,
\begin{eqnarray}
\pi_{2} & = & \frac{\pi_{1}(1-p_{ii})}{1-p_{jj}}\label{eq:3}
\end{eqnarray}

Substitute \ref{eq:3} into Equation \ref{eq:2}, then
\begin{eqnarray*}
\pi_{1} & = & \frac{1-p_{jj}}{2-p_{ii}-p_{jj}}
\end{eqnarray*}
\end{proof}
A coefficient of a predictor variable in the Markov switching model
can have either different values in different state or a constant
value in all state. The variable which have the former behavior is
said to have a \emph{switching effect}. Likewise, the variable which
have the same coefficient in all states is the variable that does
not have a switching effect, or said to have a \emph{non-switching
effect.} 

A generalized form of Equation \ref{eq:general_mswm} can be defined
as \citep{perlin2015ms_regress}

\begin{equation}
y_{t}=X_{t}^{ns}\alpha_{t}+X_{t}^{s}\beta_{S_{t}}+\varepsilon_{t}
\end{equation}

where, 
\begin{labeling}{00.00.0000}
\item [{$X_{t}^{ns}$}] contains all predictor variables that have non-switching
effect of the time series at time $t$
\item [{$\alpha_{t}$}] are non-switching coefficients of the time series
at time $t$
\item [{$X_{t}^{s}$}] contains all predictor variables that have the switching
effect of the time series at time $t$
\item [{$\beta_{S_{t}}$}] are switching coefficients in state $S_{t}$,
where $S_{t}\in\{1,...,k\}$
\item [{$\varepsilon_{t}$}] follows a normal distribution with zero mean
and variance given by $\sigma_{S_{t}}^{2}$ 
\end{labeling}

\subsection{Autoregressive (AR) model}

An autoregressive model is one type of time series models used to
describe a time-varying process. The model is flexible in handling
various kinds of time series patterns. The name autoregressive comes
from how the model performs a regression of the variable against its
own previous outputs \citep{cryer1986time}. The number of autoregressive
lags (i.e., the number of prior values used in the model) is denoted
by $p$. 
\begin{defn}
An autoregressive model of order $p$ or AR(p) model can be written
as 

\[
y_{t}=c+\sum_{i=1}^{p}\phi_{i}y_{t-i}+\varepsilon_{t}
\]

where $c$ is a constant, $\phi_{i}$ are coefficients in the autoregression
and $\varepsilon_{t}$ follows a normal distribution with zero mean
and variance $\sigma^{2}$.
\end{defn}
If $p$ is equal to one, the model AR(1) is called the first order
autoregression process.

\subsection{Markov switching autoregressive model}

A Markov switching autoregressive model is an extension of a basic
Markov switching model where observations are drawn from an autoregressive
process. The model relaxes the conditional independence assumption
by allowing an observation to depend on both past observation and
a current state \citep{shannon2009formulation}. Basically, this is
the combination between the Markov switching model and the autoregressive
model.
\begin{defn}
The first order Markov switching autoregressive model is 

\[
y_{t}=X_{t}\beta_{S_{t}}+\phi_{1,S_{t}}y_{t-1}+\varepsilon_{t}
\]

where $\phi_{1,S_{t}}$ is an autoregressive coefficient of the observed
value at time $t-1$ in state $S_{t}$. $\varepsilon_{t}$ follows
a normal distribution with zero mean and variance given by $\sigma_{S_{t}}^{2}$.
\end{defn}
The structure of the model is shown in \ref{msm-ar}. It can be clearly
seen that there is a dependency at the observation level.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.6]{picture/msm-ar1}
\par\end{centering}
\caption{Model structure of Markov switching AR(1)}
\label{msm-ar}
\end{figure}

Assuming two states $S_{t}=1$ or $2$, the set of parameters that
are necessary to describe the law of probability that governs $y_{t}$
are $\theta=\{\beta_{1},\beta_{2},\phi_{1,1},\phi_{1,2},\sigma_{1}^{2},\sigma_{2}^{2},\pi_{1},\pi_{2},p_{11},p_{22}\}$. 

For the simplicity, in this thesis, the term Markov switching autoregressive
model will be addressed as the Markov switching model.

\section{Parameter estimation}

There are various ways to estimate parameters of a Markov switching
model. Methods which have been widely used are as follows: E-M algorithm
\citep{hamilton1990analysis,kim1994dynamic} uses the maximum likelihood
criterion, Segmental K-means \citep{juang1990segmental} uses K-means
algorithm and maximizes the state-optimized likelihood criterion,
and Gibbs sampling \citep{kim1999state} uses a Markov chain Monte
Carlo simulation method based on the Bayesian inference. 

In this thesis framework, the E-M algorithm is used in estimating
parameters as the algorithm gives effective results, numerically stable,
and easy to implement. \citet{ryden2008versus} compared the computational
perspective in estimating parameters between the E-M algorithm and
the Gibbs sampling. In most cases, the Gibbs sampling tended to have
less computational time than the E-M algorithm. However, the study
indicated that if the number of states was unknown and only point
estimate was sufficient, the E-M algorithm would typically be simpler
and quicker solution in computing the estimated parameters. The E-M
algorithm is briefly described below. %
\begin{comment}
Summing up this case, we first remark that if one wants to compute
only the best model without any further information on how plausible
it is relative to other ones, then the simplest solution is using
EM to compute MLEs for all candidate models and then calculating and
comparing their BICs or some other penalized likelihood criterion.
\end{comment}


\subsection{The Expectation-Maximization algorithm}

E-M algorithm is originally designed to deal with the problem of incomplete
or missing values in data \citep{dempster1977maximum}. Nevertheless,
it could be implemented in Markov switching model since the unobserved
state $S_{t}$ can be viewed as missing data values. 

The set of parameters $\theta$ are estimated by an iterative two-step
procedure. In the first step, the algorithm starts with arbitrary
initial parameters, and then finds the expected values of the state
process from the given observations. In the second step of the iterative
procedure, a new maximum likelihood from the derived parameters in
the previous step is calculated. These two steps are repeated until
the maximum value of the likelihood function is reached or has converged
\citep{janczura2012efficient}. The two steps are known as the E-step
and the M-step. \ref{em} illustrates the process of the E-M algorithm.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.55]{picture/em}
\par\end{centering}
\caption{A flowchart showing the process of the Expectation-Maximization algorithm.
The algorithm begins with a set of initial values. The E-step is performed
by computing a filtering and smoothing algorithm. Then, the M-step
is performed. Iterating both steps until convergence.}

\label{em}
\end{figure}


\subsubsection{E-step}

In this step, $\theta^{(n)}$ is the derived set of parameters in
M-step from the previous iteration, and $n$ is a current iteration
in the algorithm. The available observations of time $t-1$ is denoted
as $\Omega_{t-1}=(y_{1},y_{2},...,y_{t-1})$. The general idea of
this step is to calculate the expectation of $S_{t}$ under the current
estimation of the parameters. The obtained result is called smoothed
inferences probability, and is denoted by $P(S_{t}=j|\Omega_{T};\theta)$
where $T$ is the number of all observations in the data and $j=1,2,...,k$.
The E-step which consists of filtering and smoothing algorithm is
described as follows \citep{kim1994dynamic}:

\paragraph{Filtering}

A filtered probability is a probability of a non-observable Markov
chain being in a given state $j$ at time $t$, conditional on information
up to time $t$. The algorithm starts from $t=1$ to $t=T$. A starting
point for the first iteration where $t=1$ is chosen from arbitrary
values. The probabilities of each state given available observations
up to time $t-1$ is calculated by

\begin{equation}
P(S_{t}=j|\Omega_{t-1};\theta^{(n)})=\sum_{i=1}^{k}p_{ij}^{(n)}P(S_{t-1}=i|\Omega_{t-1};\theta^{(n)})\qquad j=1,2,...,k
\end{equation}

The conditional densities of $y_{t}$ given $\Omega_{t-1}$ are

\begin{equation}
f(y_{t}|\Omega_{t-1};\theta^{(n)})=\sum_{j=1}^{k}f(y_{t}|S_{t}=j,\Omega_{t-1};\theta^{(n)})P(S_{t}=j|\Omega_{t-1};\theta^{(n)})
\end{equation}

where $f(y_{t}|S_{t},\Omega_{t-1};\theta)=\frac{1}{\sqrt{2\pi\sigma_{S_{t}}^{2}}}exp\left\{ -\frac{(y_{t}-\beta_{S_{t}})^{2}}{2\sigma_{S_{t}}^{2}}\right\} $
is the likelihood function in each state for time $t$. This is simply
a Gaussian probability density function.

Then, with the new observation at time $t$, the probabilities of
each state are updated by using Bayes' rule as shown below

\begin{equation}
P(S_{t}=j|\Omega_{t};\theta^{(n)})=\frac{f(y_{t}|S_{t}=j,\Omega_{t-1};\theta^{(n)})P(S_{t}=j|\Omega_{t-1};\theta^{(n)})}{f(y_{t}|\Omega_{t-1};\theta^{(n)})}\label{eq:fProb}
\end{equation}

The process above is computed iteratively until all the observation
is reached i.e., $t=T$.

\begin{comment}
The joint conditional density function of $y_{t},$$S_{t-1}$and $S_{t}$
given $\Omega_{\ensuremath{t-1}}$ are

\[
f(y_{t},S_{t-1}=i,S_{t}=j|\Omega_{t-1};\theta^{(n)})=f(y_{t}|S_{t-1}=i,S_{t=}j,\Omega_{t-1};\theta^{(n)})P(S_{t-1}=i,S_{t}=j|\Omega_{t-1};\theta^{(n)})
\]

and 

\[
P(S_{t-1}=i,S_{t}=j|\Omega_{t};\theta^{(n)})=\frac{f(y_{t},S_{t-1}=i,S_{t}=j|\Omega_{t-1};\theta^{(n)})}{f(y_{t}|\Omega_{t-1};\theta^{(n)})}
\]
\end{comment}


\paragraph{Smoothing}

A smoothed probability is a probability of a non-observable Markov
chain being in state $j$ at time $t$, conditional on all available
information. The algorithm iterates over $t=T-1,T-2,...,1$. The starting
values are obtained from the final iteration of the filtered probabilities.

By noting that

\begin{align}
P(S_{t}=j|S_{t+1}=i,\Omega_{T};\theta^{(n)}) & \thickapprox P(S_{t}=j|S_{t+1}=i,\Omega_{t};\theta^{(n)})\nonumber \\
 & =\frac{P(S_{t}=j,S_{t+1}=i|\Omega_{t};\theta^{(n)})}{P(S_{t+1}=i|\Omega_{t};\theta^{(n)})}\nonumber \\
 & =\frac{P(S_{t}=j|\Omega_{t};\theta^{(n)})p_{ij}^{(n)}}{P(S_{t+1}=i|\Omega_{t};\theta^{(n)})}
\end{align}

and

\begin{equation}
P(S_{t}=j|\Omega_{T};\theta^{(n)})=\sum_{i=1}^{k}P(S_{t}=j,S_{t+1}=i|\Omega_{T};\theta^{(n)})
\end{equation}

then, the smoothed probabilities can be expressed as

\begin{equation}
P(S_{t}=j|\Omega_{T};\theta^{(n)})=\sum_{i=1}^{k}\frac{P(S_{t+1}=i|\Omega_{T};\theta^{(n)})P(S_{t}=j|\Omega_{t};\theta^{(n)})p_{ij}^{(n)}}{P(S_{t+1}=i|\Omega_{t};\theta^{(n)})}
\end{equation}


\paragraph{Full log-likelihood}

Once the filtered probabilities are estimated, there is enough necessary
information to compute the full log-likelihood function.

\begin{equation}
\ln L(\theta)=\sum_{t=1}^{T}\ln(f(y_{t}|\Omega_{t-1};\theta^{(n)})=\sum_{t=1}^{T}\ln\sum_{j=1}^{k}((f(y_{t}|S_{t}=j,\Omega_{t-1};\theta^{(n)})P(S_{t}=j|\Omega_{t-1}))\label{eq:loglik}
\end{equation}

This is simply a weighted average of the likelihood function in each
state. The probabilities of states are considered as weights.

\subsubsection{M-step}

The new estimated model parameters $\theta^{(n+1)}$ are obtained
by finding a set of parameters that maximizes Equation \ref{eq:loglik}.
This new set of parameters is more precise than the previous estimated
value of the maximum likelihood. $\theta^{(n+1)}$ serves as a set
of parameters in the next iteration of the E-step. 

Each individual parameter in $\theta^{(n+1)}$ are taken from its
maximum value, which is determined by taking the partial derivative
of the log-likelihood function with respect to each parameter. Generally,
this process is similar to the standard maximum likelihood estimation.
However, it has to be weighted by the smoothed probabilities because
each observation $y_{t}$ contains probability from each $k$ states. 

\subsubsection{Convergence of the E-M algorithm}

The E- and M-step are iteratively computed until the algorithm converges.
The algorithm will terminate when the different between the previous
and current estimate values is less than a specific value. This specific
value called a \emph{stopping criterion} needs to be specified beforehand.
The convergence is assured since the value of the log-likelihood function
will increase in each iteration. However, the E-M algorithm does not
guarantee to always converge to a global maximum. The convergence
of the algorithm is also likely to be only a local maxima. 

\section{State prediction}

The package used for performing the Markov switching model does not
provide a function to predict the most probable state for the new
observation. Therefore, the state prediction function is implemented
as an additional function in the package for this analysis (see \ref{sec:MSwM-Package}).

The probabilities of being in state $j$ at time $T+1$ on a basis
of the current information are computed by performing the filtering
algorithm in the E-step of E-M algorithm. The filtered probabilities
are

\[
P(S_{T+1}=j|\Omega_{T+1};\theta)=\frac{f(y_{T+1}|S_{T+1}=j,\Omega_{T};\theta)P(S_{T+1}=j|\Omega_{T};\theta)}{f(y_{T+1}|\Omega_{T};\theta)}
\]

This is Equation \ref{eq:fProb} where $t=T+1$. Then, the new observation
at time $T+1$ is said to be in the state $j$ if it has the highest
probability.

\section{Model selection}

One of the most difficult tasks when modeling the Markov switching
model is to decide on the number of states \citep{rabiner1986introduction}.
The analysis will be conducted on a trial and error basis before settling
on the most appropriate size of the model. In this study, several
Markov switching models with different setting will be carried out.
First, the number of states $k$ for the model will be chosen. Then,
the number of switching coefficients in the model will be decided
based on the selected number of states. Models will be selected based
on the quality of the model. 

Model selection is a task of selecting the best model for a given
set of data. The Bayesian Information Criterion (BIC) is widely employed
in the applied literature, and proved to be useful in selecting the
model among a finite set of models (e.g., \citet{leroux1992maximum}
used BIC to select the number of states $k$). It is also known as
Schwarz Information Criterion \citep{schwarz1978estimating}. Model
which has a lower value of BIC is preferred. 

\[
\mathrm{BIC}=-2\ln(L(\hat{\theta}))+m\cdot\ln(T)
\]

where $L(\hat{\theta})$ represents the maximized value of the likelihood
function, $T$ is the number of observations, and $m$ is the number
of parameters to be estimated in the model. While including more parameters
or terms will result in a higher likelihood, it can also lead to an
overfitting. BIC attempts to reduce the risk of overfitting by taking
into account the number of parameters in the model. BIC can, therefore,
heavily penalize a complex model. 

\section{Non-parametric analysis}

A parametric analysis outperforms a non-parametric analysis if the
applied data belong to a known distribution family. However, a parametric
test does not perform well in detecting change points of an unknown
underlying distribution \citep{sharkey2014nonparametric}. Applying
a non-parametric analysis to a real-world process gives a real advantage
to the analysis. Data collected from a real-world process usually
do not have a well-defined structure, which are more suitable to be
applied with the non-parametric analysis that is not too restrictive
\citep{hawkins2010nonparametric}. For this reason, the non-parametric
analysis is implemented in order to get a rough idea of the change
point location in this thesis framework. The obtained result is also
compared with the result from using the Markov switching model.

\subsubsection*{E-divisive}

An \emph{ecp}\footnote{https://cran.r-project.org/web/packages/ecp/index.html}
is an extension package in \emph{R} which mainly focuses on computing
a non-parametric test for multiple change point analysis. This change
point method is applicable to both univariate and multivariate time
series. The fundamental idea of the package is based on the hierarchical
clustering approach \citep{james2013ecp}. 

An E-divisive method is an algorithm in the \emph{ecp} package. This
algorithm performs a divisive clustering in order to estimate change
points. The E-divisive recursively partitions a time series and estimates
a single change point at each iteration. Consequently, the new change
point is located in each iteration, which divides the time series
into different segments. The algorithm also uses a permutation test
to compute the statistical significance of an estimated change point.
The computational time of the E-divisive algorithm is $O(kT^{2})$,
where $k$ is the number of estimated change points and $T$ is the
number of observations in the time series data. More details about
the estimation is described in \citet{matteson2014nonparametric}. 

\section{Simulation study for model evaluation \label{sec:Simulation}}

The state of the CPU utilization in a real data is unknown in the
study. As a consequence, an accuracy of the Markov switching model
and the E-divisive method cannot be computed, and the comparison between
both methods can hardly be made. One possible solution to test how
effective both methods are, and to verify how well the implemented
state prediction function performs is to use a simulation technique.
The dataset that consists of two predictor variables and one response
variable with already known states is simulated. The actual models
of each state are 

\[
y_{t}=\begin{cases}
\begin{array}{c}
10+0.6X_{1,t}-0.9X{}_{2,t}+0.5y_{t-1}+\varepsilon_{t}^{(1)}\\
2+0.8X_{1,t}+0.2y_{t-1}+\varepsilon_{t}^{(2)}\\
-12+0.7X_{1,t}+0.2X{}_{2.t}-0.2y_{t-1}+\varepsilon_{t}^{(3)}
\end{array} & \begin{array}{c}
\varepsilon_{t}^{(1)}\sim N(0,1);\quad\mathrm{Normal}\\
\varepsilon_{t}^{(2)}\sim N(2,0.5);\quad\mathrm{Bad}\\
\varepsilon_{t}^{(3)}\sim N(1,1);\quad\mathrm{Good}
\end{array}\end{cases}
\]

where, 
\begin{labeling}{00.00.0000}
\item [{$y_{t}$}] is assumed to be a value of a CPU usage of the time
series at time $t$
\item [{$X_{1,t}$}] is a predictor variable generated by a uniform distribution
on $[50,200]$ of the time series at time $t$
\item [{$X_{2,t}$}] is a predictor variable generated by a uniform distribution
on $[0,50]$ of the time series at time $t$
\end{labeling}
There are two simulated datasets \textendash{} Dataset 1 and Dataset
2 \textendash{} and each of them contains 500 observations. Both datasets
have different time periods where the switches between states occur.
The simulated Dataset 1 has a longer duration to remain in its own
state before switching to the other states than the simulated Dataset
2. \ref{sim_data} and \ref{sim_data2} present plots of $y$ over
a period of time, and the period where observations in the data belong
to one of the states for the first and second simulated data, respectively.

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.35]{picture/sim1}
\par\end{centering}
\caption{\emph{Top:} A simulated data of Dataset 1 where $y$ variable is the
response variable. \emph{Bottom:} The period in the time series when
observation is in each state.}
\label{sim_data}
\end{figure}

\begin{figure}[H]
\begin{centering}
\includegraphics[scale=0.35]{picture/sim2}
\par\end{centering}
\caption{\emph{Top:} A simulated data of Dataset 2 where $y$ variable is the
response variable. \emph{Bottom:} The period in the time series when
observation is in each state.}
\label{sim_data2}
\end{figure}


